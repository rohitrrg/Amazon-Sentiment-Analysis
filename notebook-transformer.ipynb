{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bz2\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('dataset/7/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('dataset/7/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def load_extract(file):\n",
    "    texts, labels = [], []\n",
    "    for line in tqdm(file, desc=\"Extracting and Loading Data\", ):\n",
    "        x = line.decode('utf-8')\n",
    "        labels.append(int(x[9]) - 1) \n",
    "        texts.append(x[10:].strip()) \n",
    "    print('Done !') \n",
    "    return np.array(labels), texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting and Loading Data: 3600000it [01:03, 57141.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting and Loading Data: 400000it [00:07, 55888.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "train_labels, train_texts = load_extract(train_file)\n",
    "test_labels, test_texts = load_extract(test_file)\n",
    "\n",
    "# remove unwanted large data variables\n",
    "del train_file; del test_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600000 400000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(texts):\n",
    "    temp_texts = []\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Cleaning Texts\"):\n",
    "\n",
    "        # Replace digits with '0'\n",
    "        text = re.sub(r'\\d', '0', text)\n",
    "\n",
    "        # Remove links and URLs\n",
    "        if 'www.' in text or 'http:' in text or 'https:' in text or '.com' in text:\n",
    "            text = re.sub(r\"(?:https?://|www\\.)\\S+|\\b\\S*\\.com\\S*\", \" \", text)\n",
    "\n",
    "        # Remove non-alphabetic characters (except spaces)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "        # Remove extra spaces and strip leading/trailing whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        temp_texts.append(text)\n",
    "\n",
    "    return temp_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3600000/3600000 [01:52<00:00, 32088.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400000/400000 [00:12<00:00, 32479.27it/s]\n"
     ]
    }
   ],
   "source": [
    "print('\\nCleaning Training data')\n",
    "train_texts = clean_texts(train_texts)\n",
    "print('\\nCleaning Test data')\n",
    "test_texts = clean_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stuning even for the non gamer This sound track was beautiful It paints the senery in your mind so well I would recomend it even to people who hate vid game music I have played the game Chrono Cross but out of all of the games I have ever played it has the best music It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras It would impress anyone who cares to listen',\n",
       " 'The best soundtrack ever to anything I m reading a lot of reviews saying that this is the best game soundtrack and I figured that I d write a review to disagree a bit This in my opinino is Yasunori Mitsuda s ultimate masterpiece The music is timeless and I m been listening to it for years now and its beauty simply refuses to fade The price tag on this is pretty staggering I must say but if you are going to buy any cd for this much money this is the only one that I feel would be worth every penny',\n",
       " 'Amazing This soundtrack is my favorite music of all time hands down The intense sadness of Prisoners of Fate which means all the more if you ve played the game and the hope in A Distant Promise and Girl who Stole the Star have been an important inspiration to me personally throughout my teen years The higher energy tracks like Chrono Cross Time s Scar Time of the Dreamwatch and Chronomantique indefinably remeniscent of Chrono Trigger are all absolutely superb as well This soundtrack is amazing music probably the best of this composer s work I haven t heard the Xenogears soundtrack so I can t say for sure and even if you ve never played the game it would be worth twice the price to buy it I wish I could give it stars',\n",
       " 'Excellent Soundtrack I truly like this soundtrack and I enjoy video game music I have played this game and most of the music on here I enjoy and it s truly relaxing and peaceful On disk one my favorites are Scars Of Time Between Life and Death Forest Of Illusion Fortress of Ancient Dragons Lost Fragment and Drowned Valley Disk Two The Draggons Galdorb Home Chronomantique Prisoners of Fate Gale and my girlfriend likes ZelbessDisk Three The best of the three Garden Of God Chronopolis Fates Jellyfish sea Burning Orphange Dragon s Prayer Tower Of Stars Dragon God and Radical Dreamers Unstealable Jewel Overall this is a excellent soundtrack and should be brought by those that like video game music Xander Cross',\n",
       " 'Remember Pull Your Jaw Off The Floor After Hearing it If you ve played the game you know how divine the music is Every single song tells a story of the game it s that good The greatest songs are without a doubt Chrono Cross Time s Scar Magical Dreamers The Wind The Stars and the Sea and Radical Dreamers Unstolen Jewel Translation varies This music is perfect if you ask me the best it can be Yasunori Mitsuda just poured his heart on and wrote it down on paper',\n",
       " 'an absolute masterpiece I am quite sure any of you actually taking the time to read this have played the game at least once and heard at least a few of the tracks here And whether you were aware of it or not Mitsuda s music contributed greatly to the mood of every single minute of the whole game Composed of CDs and quite a few songs I haven t an exact count all of which are heart rendering and impressively remarkable this soundtrack is one I assure you you will not forget It has everything for every listener from fast paced and energetic Dancing the Tokage or Termina Home to slower and more haunting Dragon God to purely beautifully composed Time s Scar to even some fantastic vocals Radical Dreamers This is one of the best videogame soundtracks out there and surely Mitsuda s best ever',\n",
       " 'Buyer beware This is a self published book and if you want to know why read a few paragraphs Those star reviews must have been written by Ms Haddon s family and friends or perhaps by herself I can t imagine anyone reading the whole thing I spent an evening with the book and a friend and we were in hysterics reading bits and pieces of it to one another It is most definitely bad enough to be entered into some kind of a worst book contest I can t believe Amazon even sells this kind of thing Maybe I can offer them my th grade term paper on To Kill a Mockingbird a book I am quite sure Ms Haddon never heard of Anyway unless you are in a mood to send a book to someone as a joke stay far far away from this one',\n",
       " 'Glorious story I loved Whisper of the wicked saints The story was amazing and I was pleasantly surprised at the changes in the book I am not normaly someone who is into romance novels but the world was raving about this book and so I bought it I loved it This is a brilliant story because it is so true This book was so wonderful that I have told all of my friends to read it It is not a typical romance it is so much more Not reading this book is a crime becuase you are missing out on a heart warming story',\n",
       " 'A FIVE STAR BOOK I just finished reading Whisper of the Wicked saints I fell in love with the caracters I expected an average romance read but instead I found one of my favorite books of all time Just when I thought I could predict the outcome I was shocked The writting was so descriptive that my heart broke when Julia s did and I felt as if I was there with them instead of just a distant reader If you are a lover of romance novels then this is a must read Don t let the cover fool you this book is spectacular',\n",
       " 'Whispers of the Wicked Saints This was a easy to read book that made me want to keep reading on and on not easy to put down It left me wanting to read the follow on which I hope is coming soon I used to read a lot but have gotten away from it This book made me want to read again Very enjoyable']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\anaconda3\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: \"negative\", 1: \"positive\"}\n",
    "num_labels = len(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3240000\n",
      "Number of validation samples: 360000\n",
      "Number of test samples: 400000\n",
      "Number of sentiment classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_texts)}\")\n",
    "print(f\"Number of validation samples: {len(valid_texts)}\")\n",
    "print(f\"Number of test samples: {len(test_texts)}\")\n",
    "print(f\"Number of sentiment classes: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use DistilBERT for its balance of performance and speed.\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset\n",
    "class AmazonReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimized Tokenization for Large Datasets ---\n",
    "def tokenize_and_save(texts=None, labels=None, tokenizer, max_length, file_prefix):\n",
    "    \"\"\"\n",
    "    Tokenizes text data in batches and saves the encodings and labels.\n",
    "    If files exist, loads them instead of re-tokenizing.\n",
    "    \"\"\"\n",
    "    encodings_path = f\"encodings/{file_prefix}_encodings.pt\"\n",
    "    labels_path = f\"encodings/{file_prefix}_labels.pt\"\n",
    "\n",
    "    if os.path.exists(encodings_path) and os.path.exists(labels_path):\n",
    "        print(f\"Loading pre-tokenized data from {file_prefix}...\")\n",
    "        encodings = torch.load(encodings_path)\n",
    "        labels = torch.load(labels_path)\n",
    "        # Convert tensors back to lists/dicts if necessary for the Dataset class\n",
    "        encodings = {key: val.tolist() for key, val in encodings.items()}\n",
    "        labels = labels.tolist()\n",
    "        return encodings, labels\n",
    "    else:\n",
    "        print(f\"Tokenizing data for {file_prefix}...\")\n",
    "        # Process in batches to manage memory and potentially speed up\n",
    "        batch_size = 1000 # Adjust based on your system's memory\n",
    "        all_input_ids = []\n",
    "        all_attention_mask = []\n",
    "        all_token_type_ids = [] # DistilBERT doesn't use token_type_ids, but good to keep general\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=max_length)\n",
    "            all_input_ids.extend(batch_encodings['input_ids'])\n",
    "            all_attention_mask.extend(batch_encodings['attention_mask'])\n",
    "            if 'token_type_ids' in batch_encodings:\n",
    "                all_token_type_ids.extend(batch_encodings['token_type_ids'])\n",
    "\n",
    "        encodings = {\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_mask\n",
    "        }\n",
    "        if all_token_type_ids: # Only add if present\n",
    "            encodings['token_type_ids'] = all_token_type_ids\n",
    "\n",
    "        # Save tokenized data\n",
    "        torch.save({key: torch.tensor(val) for key, val in encodings.items()}, encodings_path)\n",
    "        torch.save(torch.tensor(labels), labels_path)\n",
    "        print(f\"Tokenized data saved to {file_prefix}_encodings.pt and {file_prefix}_labels.pt\")\n",
    "        return encodings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "# valid_texts = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n",
    "# test_texts = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-tokenized data from train_data...\n",
      "Loading pre-tokenized data from val_data...\n",
      "Loading pre-tokenized data from test_data...\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and load/save datasets\n",
    "train_encodings, train_labels = tokenize_and_save(train_texts, train_labels, tokenizer, 128, \"train_data\")\n",
    "val_encodings, val_labels = tokenize_and_save(valid_texts, valid_labels, tokenizer, 128, \"val_data\")\n",
    "test_encodings, test_labels = tokenize_and_save(test_texts, test_labels, tokenizer, 128, \"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset objects\n",
    "train_dataset = AmazonReviewDataset(train_encodings, train_labels)\n",
    "valid_dataset = AmazonReviewDataset(val_encodings, valid_labels)\n",
    "test_dataset = AmazonReviewDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset preparation complete.\n",
      "Example of tokenized input_ids (first training sample): tensor([  101,  4895, 14854,  2100,  2003,  2157,  4895, 14854,  2100, 11771,\n",
      "         1045,  2064,  1056,  2903,  1996,  2204,  4391,  1996,  3772,  2001,\n",
      "         2919,  1045,  1049,  5373,  1037,  5470,  1997,  4748, 23144,  2638,\n",
      "         3347, 26401,  2021,  1045,  2347,  1056,  7622,  2007,  2009,  2009,\n",
      "        11471,  2033, 10021,  1998,  1045,  2018,  2000,  2428,  2954,  2000,\n",
      "         2994,  8300,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Example of attention_mask (first training sample): tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Example of label (first training sample): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset preparation complete.\")\n",
    "print(f\"Example of tokenized input_ids (first training sample): {train_dataset[0]['input_ids']}\")\n",
    "print(f\"Example of attention_mask (first training sample): {train_dataset[0]['attention_mask']}\")\n",
    "print(f\"Example of label (first training sample): {train_dataset[0]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded with a classification head.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained DistilBERT model with a classification head\n",
    "# num_labels should match the number of unique sentiment categories you have.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "print(\"\\nModel loaded with a classification head.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=128,  # batch size per device during training\n",
    "    per_device_eval_batch_size=128,   # batch size per device during evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,               # log every 100 steps\n",
    "    eval_strategy=\"epoch\",     # evaluate each epoch\n",
    "    save_strategy=\"best\",            # save model checkpoint each epoch\n",
    "    save_total_limit=1,              \n",
    "    load_best_model_at_end=True,     # load the best model (according to eval_loss) at the end of training\n",
    "    metric_for_best_model=\"eval_loss\", # metric to use to compare models\n",
    "    report_to=\"none\"                 # Disable reporting to W&B, MLflow etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be passed to the Trainer for calculating metrics during evaluation.\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # the callback for computing metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15499' max='151875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 15499/151875 6:11:40 < 54:30:47, 0.69 it/s, Epoch 0.31/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting model training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating model on the test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"Test Set Evaluation Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = \"./fine_tuned_sentiment_model\"\n",
    "model.save_pretrained(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFine-tuned model and tokenizer saved to: {output_model_dir}\")\n",
    "print(\"You can now load this model for inference in your Streamlit app.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"galaxy tab S9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:141.0) Gecko/20100101 Firefox/141.0',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import AmazonProductDetails\n",
    "\n",
    "scrapy = AmazonProductDetails()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = scrapy.get_product_details('macbook air')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = product['reviews']['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nGreat device even though it's a 2020 model. The screen feels absolutely premium and the overall experience is fantastic. Battery lasts longer than other laptops at this price, speed is immaculate, the design is old but looks simple and feels good. If you're searching for a laptop for just browsing, streaming and studying, this is a must buy for this price <333\\n\",\n",
       " \"\\n1. Performance:The M1 MacBook Air is incredibly fast and efficient. The M1 chip's performance is on par with or even surpasses many Intel-based MacBook Pros, making it one of the most powerful laptops in its class. Whether you're a casual user, a creative professional, or a developer, it handles most tasks with ease.2. Battery Life:One of the standout features of the M1 MacBook Air is its impressive battery life. Users report getting 12-15 hours of real-world use, which is a significant improvement over previous Intel-based MacBook Air models. You can go an entire workday or more without needing to charge it.3. Fanless Design:The M1 MacBook Air is fanless, which means it operates silently. This is a big plus for those who value a quiet working environment.4. Portability:The MacBook Air remains one of the most portable laptops on the market. It's lightweight, slim, and has a high-quality Retina display. Its build quality is top-notch.5. Compatibility:The transition from Intel to Apple's M1 architecture meant some software might not be immediately compatible. However, Apple has made substantial progress in optimizing and making software compatible with the M1, including popular applications and operating systems.6. Rosetta 2:Apple's Rosetta 2 technology allows older Intel-based apps to run on the M1 Macs, and it does so quite efficiently. Still, some applications may not be as optimized or as fast as native M1 apps.7. Price:The M1 MacBook Air offers excellent value for the performance you get, especially when compared to some Intel-based MacBooks.8. Integrated Graphics:The integrated GPU in the M1 chip is impressive and can handle a range of tasks, including photo and video editing. However, if you're a professional video editor or a gamer, you might want to consider a MacBook Pro with dedicated graphics.9. Limited Port Selection:The MacBook Air features only two USB-C/Thunderbolt 3 ports and a headphone jack. This could be a limitation if you need more ports.10. Upgradability:The M1 MacBook Air is not user-upgradable. So, you'll need to choose the right configuration at the time of purchase.\\n\",\n",
       " '\\nThe MacBook delivers top-notch performance with a sleek, premium design.Battery life is excellent, and the display is crisp and vibrant.Perfect for work, creativity, and everyday use â€” truly worth the investment!\\n',\n",
       " \"\\n*This is my secondary device*As a writer churning out 3,000-4,000 words daily, the MacBook Air M1 serves as a reliable secondary device. While it's not my primary workstation, it effortlessly handles my writing needs. The keyboard, with its tactile and responsive keys, is a delight to type on. However, transitioning from a Windows machine, I did encounter a learning curve with the unique macOS shortcuts.For light tasks like browsing, emailing, and word processing, the M1's snappy performance is impressive. However, when pushing it with multiple heavy-duty applications, the 8GB RAM can sometimes become a bottleneck. If you're seeking a long-term investment, the M2 variant with more RAM would be a wiser choice.Nevertheless, considering its price point of around 53k in 2024, the MacBook Air M1 offers exceptional value for money. It's a solid choice for writers, students, and casual users who need a reliable and portable device.\\n\",\n",
       " \"\\nThe most beautiful feeling for a beginner mac user. I got this laptop for â‚¹55,000 in the prime day sale. It's so smooth and it's screen is fantastic. It feels really premium while you unbox it and take it on your lap. I am a student so will use it for study and assignment purposes. Will also try to learn video editing here. It's super fast as soon as I opened it, it started within 5 seconds...Overall its a great laptop. I got it delivered today and I am looking forward for this wonderful machine..\\n\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\anaconda3\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentiment_analyzer import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Tokenizer and Model for model fine_tuned_sentiment_model\n"
     ]
    }
   ],
   "source": [
    "sa = SentimentAnalyzer(model_name='fine_tuned_sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Positive', 'Positive', 'Positive', 'Positive', 'Positive']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.predict_sentiment(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nUpgraded to iPhone 16 Pro and totally loving it. Super smooth performance, excellent camera, and stunning display. Battery easily lasts a day. Feels very premium in hand. Bit pricey, but worth it for the experience\\n',\n",
       " '\\nThis product is good genuine product no heating issue lcd quality is very good\\n',\n",
       " '\\nAwesome phone\\n',\n",
       " '\\n\\n\\n\\n\\n                    The media could not be loaded.\\n                \\n\\n\\n\\nIt did some lags while using it and i donâ€™t even play games on my phone. It is good from the battery life perspective.\\n',\n",
       " '\\nSuperb phone,,,\\n',\n",
       " '\\nThe Product is Good but Extremely Expensive\\n',\n",
       " '\\nI was actually hesitant to order an iPhone from Amazon because there have been instances where the product wasnâ€™t genuine. However, I was finally relieved to see that the package was genuine, and the iPhone itself was the real deal. ðŸ˜‚I ordered the black model, and the color is sleek. I love it! ðŸ–¤If youâ€™re coming from an older iPhone model like the 13 Pro or earlier, or if youâ€™re new to the Apple ecosystem, this is a great upgrade. However, I donâ€™t think itâ€™s worth upgrading from an iPhone 15 Pro if youâ€™re a casual user. Thereâ€™s not much of a difference between the two, except that youâ€™ll be holding the latest iPhone. ðŸ˜‰\\n',\n",
       " '\\nThe delivery person seemed a little cranky and kinda rude through out the old phone trade-in process. The rest of the experience otherwise was great.The package arrived in a good condition. I was skeptical about making a major purchase off of Amazon but phew, it just all happened without any issues.Worth the upgrade if going from iPhone 13 or older, cause thereâ€™s a lot of changes from the previous slightly older models.The battery initially seemed to be draining pretty fast, but once I updated the phone to the latest ios somehow the battery life got better. Not sure if itâ€™s with all the iPhones, just an observation from my side.\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
